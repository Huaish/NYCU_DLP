{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "x = torch.randn(64, 1000, device=device)\n",
    "y = torch.randn(64, 10, device=device)\n",
    "\n",
    "w1 = torch.randn(1000, 100, device=device)\n",
    "w2 = torch.randn(100, 10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30570998.)\n",
      "tensor(25449520.)\n",
      "tensor(25784770.)\n",
      "tensor(27259042.)\n",
      "tensor(26666554.)\n",
      "tensor(22452662.)\n",
      "tensor(15865979.)\n",
      "tensor(9667770.)\n",
      "tensor(5438052.)\n",
      "tensor(3067293.)\n",
      "tensor(1848839.)\n",
      "tensor(1229853.5000)\n",
      "tensor(899446.5000)\n",
      "tensor(706773.8125)\n",
      "tensor(582078.7500)\n",
      "tensor(493572.6250)\n",
      "tensor(425997.1875)\n",
      "tensor(371837.8125)\n",
      "tensor(327192.9375)\n",
      "tensor(289580.9375)\n",
      "tensor(257480.5312)\n",
      "tensor(229841.4219)\n",
      "tensor(205885.2188)\n",
      "tensor(184994.1562)\n",
      "tensor(166698.5938)\n",
      "tensor(150604.0625)\n",
      "tensor(136395.0312)\n",
      "tensor(123810.7031)\n",
      "tensor(112622.1094)\n",
      "tensor(102660.9453)\n"
     ]
    }
   ],
   "source": [
    "for t in range(30):\n",
    "    h = x.mm(w1) # 64 x 100\n",
    "    h_relu = h.clamp(min=0) # 64 x 100\n",
    "    y_pred = h_relu.mm(w2) # 64 x 10\n",
    "    loss = (y_pred - y) # 64 x 10\n",
    "    \n",
    "    grad_y_pred = 2.0 * loss # 64 x 10\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred) # 100 x 10\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t()) # 64 x 100\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "    print(loss.pow(2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement our own deep learning model by using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 1000, device=device)\n",
    "y = torch.randn(64, 10, device=device)\n",
    "loader = DataLoader(TensorDataset(x,y), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.linear1(x)\n",
    "        h_relu = torch.nn.functional.relu(h)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in=1000, H=100, D_out=10)\n",
    "model = model.to(device=device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f5eb0ef2180>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941053032875061\n",
      "Parameter: linear1.weight, Gradient norm: 2.621346950531006\n",
      "Parameter: linear1.weight, Gradient norm: 2.621346950531006\n",
      "Parameter: linear1.bias, Gradient norm: 0.07075899094343185\n",
      "Parameter: linear1.bias, Gradient norm: 0.07075899094343185\n",
      "Parameter: linear2.weight, Gradient norm: 0.8423654437065125\n",
      "Parameter: linear2.weight, Gradient norm: 0.8423654437065125\n",
      "Parameter: linear2.bias, Gradient norm: 0.18831156194210052\n",
      "Parameter: linear2.bias, Gradient norm: 0.18831156194210052\n",
      "0.8698580861091614\n",
      "Parameter: linear1.weight, Gradient norm: 2.808884620666504\n",
      "Parameter: linear1.weight, Gradient norm: 2.808884620666504\n",
      "Parameter: linear1.bias, Gradient norm: 0.08466572314500809\n",
      "Parameter: linear1.bias, Gradient norm: 0.08466572314500809\n",
      "Parameter: linear2.weight, Gradient norm: 0.8893697261810303\n",
      "Parameter: linear2.weight, Gradient norm: 0.8893697261810303\n",
      "Parameter: linear2.bias, Gradient norm: 0.1852453052997589\n",
      "Parameter: linear2.bias, Gradient norm: 0.1852453052997589\n",
      "1.3996388912200928\n",
      "Parameter: linear1.weight, Gradient norm: 3.596364974975586\n",
      "Parameter: linear1.weight, Gradient norm: 3.596364974975586\n",
      "Parameter: linear1.bias, Gradient norm: 0.10645096004009247\n",
      "Parameter: linear1.bias, Gradient norm: 0.10645096004009247\n",
      "Parameter: linear2.weight, Gradient norm: 1.0116454362869263\n",
      "Parameter: linear2.weight, Gradient norm: 1.0116454362869263\n",
      "Parameter: linear2.bias, Gradient norm: 0.21025450527668\n",
      "Parameter: linear2.bias, Gradient norm: 0.21025450527668\n",
      "0.9350149035453796\n",
      "Parameter: linear1.weight, Gradient norm: 2.7623603343963623\n",
      "Parameter: linear1.weight, Gradient norm: 2.7623603343963623\n",
      "Parameter: linear1.bias, Gradient norm: 0.08092395216226578\n",
      "Parameter: linear1.bias, Gradient norm: 0.08092395216226578\n",
      "Parameter: linear2.weight, Gradient norm: 0.8713314533233643\n",
      "Parameter: linear2.weight, Gradient norm: 0.8713314533233643\n",
      "Parameter: linear2.bias, Gradient norm: 0.21055541932582855\n",
      "Parameter: linear2.bias, Gradient norm: 0.21055541932582855\n",
      "1.3475606441497803\n",
      "Parameter: linear1.weight, Gradient norm: 3.464062452316284\n",
      "Parameter: linear1.weight, Gradient norm: 3.464062452316284\n",
      "Parameter: linear1.bias, Gradient norm: 0.11064118891954422\n",
      "Parameter: linear1.bias, Gradient norm: 0.11064118891954422\n",
      "Parameter: linear2.weight, Gradient norm: 1.0057631731033325\n",
      "Parameter: linear2.weight, Gradient norm: 1.0057631731033325\n",
      "Parameter: linear2.bias, Gradient norm: 0.2748394012451172\n",
      "Parameter: linear2.bias, Gradient norm: 0.2748394012451172\n",
      "0.9051774740219116\n",
      "Parameter: linear1.weight, Gradient norm: 2.6898441314697266\n",
      "Parameter: linear1.weight, Gradient norm: 2.6898441314697266\n",
      "Parameter: linear1.bias, Gradient norm: 0.08311482518911362\n",
      "Parameter: linear1.bias, Gradient norm: 0.08311482518911362\n",
      "Parameter: linear2.weight, Gradient norm: 0.7149016261100769\n",
      "Parameter: linear2.weight, Gradient norm: 0.7149016261100769\n",
      "Parameter: linear2.bias, Gradient norm: 0.14540693163871765\n",
      "Parameter: linear2.bias, Gradient norm: 0.14540693163871765\n",
      "0.93780118227005\n",
      "Parameter: linear1.weight, Gradient norm: 2.678046464920044\n",
      "Parameter: linear1.weight, Gradient norm: 2.678046464920044\n",
      "Parameter: linear1.bias, Gradient norm: 0.07083631306886673\n",
      "Parameter: linear1.bias, Gradient norm: 0.07083631306886673\n",
      "Parameter: linear2.weight, Gradient norm: 0.8238194584846497\n",
      "Parameter: linear2.weight, Gradient norm: 0.8238194584846497\n",
      "Parameter: linear2.bias, Gradient norm: 0.1854347437620163\n",
      "Parameter: linear2.bias, Gradient norm: 0.1854347437620163\n",
      "0.916191577911377\n",
      "Parameter: linear1.weight, Gradient norm: 2.7899210453033447\n",
      "Parameter: linear1.weight, Gradient norm: 2.7899210453033447\n",
      "Parameter: linear1.bias, Gradient norm: 0.09283927083015442\n",
      "Parameter: linear1.bias, Gradient norm: 0.09283927083015442\n",
      "Parameter: linear2.weight, Gradient norm: 0.9046415090560913\n",
      "Parameter: linear2.weight, Gradient norm: 0.9046415090560913\n",
      "Parameter: linear2.bias, Gradient norm: 0.24490806460380554\n",
      "Parameter: linear2.bias, Gradient norm: 0.24490806460380554\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = model(x_batch)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y_batch)\n",
    "        \n",
    "        print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f'Parameter: {name}, Gradient norm: {param.grad.norm().item()}')\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
